from haystack.evaluation.eval_run_result import EvaluationRunResult
from haystack import Document
from haystack.document_stores.in_memory import InMemoryDocumentStore
import os
import json
from haystack import Pipeline
from haystack.components.generators import HuggingFaceAPIGenerator
from haystack.components.generators.chat import OpenAIChatGenerator
import os
from haystack.dataclasses import ChatMessage
from haystack.components.builders import PromptBuilder, AnswerBuilder
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.evaluators.document_mrr import DocumentMRREvaluator
from haystack.components.evaluators.faithfulness import FaithfulnessEvaluator
from haystack.components.evaluators.sas_evaluator import SASEvaluator
from embed_document import load_json_file, extract_document_contents, embed_documents_grouped, save_database_to_disk
from eval_pipeline import load_document_store_with_embeddings
import pandas as pd 
from bert_score import score
import logging
import transformers
import pickle 
from
transformers.tokenization_utils.logger.setLevel(logging.ERROR)
transformers.configuration_utils.logger.setLevel(logging.ERROR)
transformers.modeling_utils.logger.setLevel(logging.ERROR)


if __name__ == "__main__":
    embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
    text_embedder = SentenceTransformersTextEmbedder(
        model=embedding_model)


    template = [ChatMessage.from_user("""
    Answer this question by following the format from this example:     
        Question: What types of road improvements are planned for Greenmount Road in Belleville, Illinois? 
        Answer: Additional lanes, standard overlay, bridge replacement. 
    Keep the answer short, no additional informaiton.     

    Question: {{question}}
    Answer:                                        
    """)]

    prompt_builder = PromptBuilder(template=template)

    if "OPENAI_API_KEY" not in os.environ:
        os.environ["OPENAI_API_KEY"] = "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"
    chat_generator = OpenAIChatGenerator(model="gpt-4o-mini")

    basic_rag_pipeline = Pipeline()
    # Add components to the pipeline
    basic_rag_pipeline.add_component("text_embedder", text_embedder)
    basic_rag_pipeline.add_component("prompt_builder", prompt_builder)
    basic_rag_pipeline.add_component("generator", chat_generator)
    basic_rag_pipeline.add_component("answer_builder", AnswerBuilder())

    # Now, connect the components to each other
    basic_rag_pipeline.connect("text_embedder.embedding",
                               "prompt_builder")
    basic_rag_pipeline.connect("prompt_builder.prompt", "generator.messages")
    basic_rag_pipeline.connect("generator.replies", "answer_builder.replies")

    with open('./data/querys.json', 'r') as file:
    # Load the querys into a variable
        querys = json.load(file)

    questions = querys
    responses = []
    answers = []
    documents = []
    for question in questions:
        response = basic_rag_pipeline.run(
            {"text_embedder": {"text": question}, "prompt_builder": {"question": question}, "answer_builder": {"query": question}})
        responses.append(response)
        answers.append(response["answer_builder"]["answers"][0].data)
        current_docs = response["answer_builder"]["answers"][0].documents
        documents.append([doc.content for doc in current_docs])



print(len(questions), len(answers), len(documents))

with open("./data/debug.pickle", "wb") as f:
    pickle.dump([questions, answers, documents], f, pickle.HIGHEST_PROTOCOL)
